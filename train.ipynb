{
 "cells": [
  {
   "cell_type": "raw",
   "id": "cdbd95aa-cf84-4f17-83a8-4d95714b4444",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Created on October 12, 2022\n",
    "\n",
    "The following things are implemented:\n",
    "    - Kfold cross validation avg\n",
    "    \n",
    "To implement:\n",
    "    - Stop training when no improvement\n",
    "    - Use network to crop the fish out (will need to change dataset.py, transform fct will be different for every img)\n",
    "    \n",
    "Things to think about:\n",
    "    - Decrease LR \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb87258b-6665-4bb8-b5c6-d5095d1c94ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import os\n",
    "import random\n",
    "import dataset\n",
    "import metrics\n",
    "import time\n",
    "\n",
    "import constants as cst\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import loss_fn_v2 as loss_fn\n",
    "from unet import UNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445daec6-a2cc-4069-9324-ee29b4af3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(cst.SEED)\n",
    "torch.manual_seed(cst.SEED)\n",
    "np.random.seed(cst.SEED)\n",
    "\n",
    "def predict_img(model, image, device, transform, out_threshold=0.5):\n",
    "    with torch.no_grad():\n",
    "        x = image\n",
    "        logits = model(x.to(device))\n",
    "        logits = transform(logits)\n",
    "        y_pred = nn.Softmax(dim=1)(logits)\n",
    "        proba = y_pred.detach().cpu().squeeze(0).numpy()[1, :, :]\n",
    "        return proba > out_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2b24043-29e4-4bd7-ae4e-fd93b624bde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "TERMS = [\"br2\"]\n",
    "SIZE = (384, 512)\n",
    "FOLDS = [2]    # 0 to 4 only 1 fold at a time here for computational resources limit\n",
    "\n",
    "DEVICE_NAME = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE_NAME = 'cuda:0'\n",
    "DEVICE = torch.device(DEVICE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04bf507d-a9c5-4340-8ec4-36f019448799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting term: br2\n",
      "Starting fold: 2\n",
      "Training set length: 46\n",
      "Validation set length: 11\n",
      "Pre-testing set length: 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for TERM in TERMS:\n",
    "    print(\"Starting term: \" + TERM)\n",
    "    start_term = time.time()\n",
    "    \n",
    "    DATASET = \"/notebooks/images\"\n",
    "    MASKS = \"/notebooks/\" + TERM\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize(SIZE),\n",
    "                                    transforms.Pad((0, 64, 0, 64))])\n",
    "    untransform = transforms.Compose([transforms.CenterCrop(SIZE),\n",
    "                                     transforms.Resize((1932, 2576))])\n",
    "    \n",
    "    fold_validation = []\n",
    "    fold_precision = []\n",
    "    fold_recall = []\n",
    "    fold_f1 = []\n",
    "    fold_IOU = []\n",
    "    \n",
    "    for fold in FOLDS:\n",
    "        print(\"Starting fold: {}\".format(fold))\n",
    "        start_fold = time.time()\n",
    "        \"\"\"Datasets and loaders\"\"\"\n",
    "        image_folder = DATASET\n",
    "        mask_folder = MASKS\n",
    "        training_set = dataset.ZebrafishDataset_KFold_v2(image_folder,\n",
    "                                                      mask_folder,\n",
    "                                                      actual_fold=fold,\n",
    "                                                      dataset=\"train\",\n",
    "                                                      folds=cst.FOLDS)\n",
    "        validation_set = dataset.ZebrafishDataset_KFold_v2(image_folder,\n",
    "                                                        mask_folder,\n",
    "                                                        actual_fold=fold,\n",
    "                                                        dataset=\"validate\",\n",
    "                                                        folds=cst.FOLDS)\n",
    "        testing_set = dataset.ZebrafishDataset_KFold_v2(image_folder,\n",
    "                                                     mask_folder,\n",
    "                                                     actual_fold=fold,\n",
    "                                                     dataset=\"pre-test\",\n",
    "                                                     folds=cst.FOLDS)\n",
    "\n",
    "        training_loader = torch.utils.data.DataLoader(training_set,\n",
    "                                                      batch_size=cst.BATCH_SIZE,\n",
    "                                                      shuffle=True,\n",
    "                                                      num_workers=cst.WORKERS)\n",
    "\n",
    "        validation_loader = torch.utils.data.DataLoader(validation_set,\n",
    "                                                        batch_size=cst.BATCH_SIZE,\n",
    "                                                        shuffle=True,\n",
    "                                                        num_workers=cst.WORKERS)\n",
    "\n",
    "        testing_loader = torch.utils.data.DataLoader(testing_set,\n",
    "                                                     batch_size=1,\n",
    "                                                     shuffle=True,\n",
    "                                                     num_workers=cst.WORKERS)\n",
    "        \n",
    "        model = UNET(3, 2)\n",
    "        model.to(DEVICE)\n",
    "        best_model = UNET(3, 2)\n",
    "        best_model = model\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion_string = \"CE\"\n",
    "\n",
    "        if cst.LOSS == \"Dice\":\n",
    "            print(\"Dice\")\n",
    "            criterion = loss_fn.DiceLoss()\n",
    "            criterion_string = \"DCE\"\n",
    "        if cst.LOSS == \"IOU\":\n",
    "            print(\"IOU\")\n",
    "            criterion = loss_fn.IoULoss()\n",
    "            criterion_string = \"IOU\"\n",
    "        if cst.LOSS == \"Tversky\":\n",
    "            print(\"Tversky\")\n",
    "            criterion = loss_fn.TverskyLoss(alpha=0.7, beta= 0.3)\n",
    "            criterion_string = \"Tversky\"\n",
    "        if cst.LOSS == \"Focal\":\n",
    "            print(\"Focal\")\n",
    "            criterion = loss_fn.FocalLoss(alpha=0.8, gamma= 2, reduction=\"mean\")\n",
    "            criterion_string = \"Focal\"\n",
    "\n",
    "        optimiser = torch.optim.Adam(model.parameters(), lr=cst.LEARNING_RATE, weight_decay=cst.WEIGHT_DECAY)\n",
    "        optimiser_string = \"ADAM\" + \"_\" + \"LR\" + str(cst.LEARNING_RATE) + \"_\" + \"WD\" + str(cst.WEIGHT_DECAY)\n",
    "        \n",
    "        if cst.OPTIMIZER == \"SGD\":\n",
    "            optimizer = torch.optim.SGD(model.parameters(),\n",
    "                                        lr=cst.LEARNING_RATE,\n",
    "                                        momentum=cst.MOMENTUM,\n",
    "                                        weight_decay=cst.WEIGHT_DECAY)\n",
    "            optimiser_string = \"SGD\" + \"_\" + \"LR\" + str(cst.LEARNING_RATE) + \"_\" + \"M\" + str(cst.MOMENTUM)\n",
    "            optimiser_string += \"_\" + \"WD\" + str(cst.WEIGHT_DECAY)\n",
    "        \n",
    "        params_string = \"Params\" + \"_\" + \"Epoch\" + str(cst.EPOCHS) + \"_\" + \"BS\" + str(cst.BATCH_SIZE)\n",
    "        params_string += \"_\" + \"W\" + str(cst.WORKERS)\n",
    "        \n",
    "        \"\"\"Computing validation loss before training\"\"\"\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = []\n",
    "            for images, masks, names in validation_loader:\n",
    "                images = transform(images)\n",
    "                outputs = model(images.to(DEVICE))\n",
    "                outputs = untransform(outputs)\n",
    "\n",
    "                masks = masks.type(torch.LongTensor)\n",
    "                masks = torch.squeeze(masks, 1)\n",
    "\n",
    "                vloss = criterion(outputs, masks.to(DEVICE))\n",
    "                loss = vloss.detach().item()\n",
    "                val_loss.append(loss)\n",
    "\n",
    "            loss = np.mean(val_loss)\n",
    "            # print(\"Validation loss before training: {}\".format(loss))\n",
    "            \n",
    "        best_val = loss\n",
    "        best_epoch = 0\n",
    "        last_epoch = 0\n",
    "        \n",
    "        epochs_train_losses = []\n",
    "        epochs_val_losses = []\n",
    "        for epoch in range(cst.EPOCHS):\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = []\n",
    "            for images, masks, names in training_loader:\n",
    "                images = transform(images)\n",
    "                outputs = model(images.to(DEVICE))\n",
    "                outputs = untransform(outputs)\n",
    "\n",
    "                masks = masks.type(torch.LongTensor)\n",
    "                masks = torch.squeeze(masks, 1)\n",
    "\n",
    "                tloss = criterion(outputs, masks.to(DEVICE))\n",
    "                loss = tloss.detach().item()\n",
    "                train_loss.append(loss)\n",
    "\n",
    "                optimiser.zero_grad()\n",
    "                tloss.backward()\n",
    "                optimiser.step()\n",
    "\n",
    "            loss = np.mean(train_loss)\n",
    "            epochs_train_losses.append(loss)\n",
    "            #print(\"Trained: {}\".format(loss), end=\". \")\n",
    "\n",
    "            \"\"\"Validation\"\"\"\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_loss = []\n",
    "                for images, masks, names in validation_loader:\n",
    "                    images = transform(images)\n",
    "                    outputs = model(images.to(DEVICE))\n",
    "                    outputs = untransform(outputs)\n",
    "\n",
    "                    masks = masks.type(torch.LongTensor)\n",
    "                    masks = torch.squeeze(masks, 1)\n",
    "\n",
    "                    vloss = criterion(outputs, masks.to(DEVICE))\n",
    "                    loss = vloss.detach().item()\n",
    "                    val_loss.append(loss)\n",
    "\n",
    "                loss = np.mean(val_loss)\n",
    "                epochs_val_losses.append(loss)\n",
    "                \n",
    "                if loss < best_val:\n",
    "                    best_val = loss\n",
    "                    best_model = model\n",
    "                    best_epoch = epoch+1\n",
    "                \n",
    "                if (epoch+1)%50 == 0:\n",
    "                    print(\"Epoch: \" + str(epoch+1))\n",
    "                    #print(\"Training: {}\".format(np.mean(train_loss)))\n",
    "                    print(\"Validation: {}.\".format(loss))\n",
    "                    print(\"Best: {}.\".format(best_val))\n",
    "                    \n",
    "            \"\"\"Train and validate loops over\"\"\"\n",
    "            curr = time.time()\n",
    "            curr = curr - start_term\n",
    "            secondes = curr % 60\n",
    "            minutes = (curr-secondes)/60\n",
    "            \n",
    "            last_epoch = epoch\n",
    "            \n",
    "            if minutes >= 345:\n",
    "                break\n",
    "            if (best_epoch - epoch) >= 50:\n",
    "                break\n",
    "            \n",
    "        \"\"\"All epochs are over\"\"\"\n",
    "        \n",
    "        fold_validation.append(best_val)\n",
    "        \n",
    "        model_name = TERM + '_' + cst.LOSS + \"_Fold_\" + str(fold) + \"_Epoch_\" + str(best_epoch) + \"_MaxEpochs_\" \n",
    "        model_name += str(cst.EPOCHS) + '_' + cst.OPTIMIZER + \"_LR_\" + str(cst.LEARNING_RATE) + \".pth\"\n",
    "        \n",
    "        model_filepath = os.path.join(cst.MODEL, model_name)\n",
    "        torch.save(best_model.state_dict(), model_filepath)\n",
    "        \n",
    "        curr = time.time()\n",
    "        curr = curr - start_fold\n",
    "        secondes = curr % 60\n",
    "        minutes = (curr-secondes)/60\n",
    "        \n",
    "        index = [i+1 for i in range(last_epoch+1)]\n",
    "        plt.plot(index[1:], epochs_train_losses[1:], label=\"Training\")\n",
    "        plt.plot(index[1:], epochs_val_losses[1:], label=\"Validation\")\n",
    "        plt.title(str(cst.LOSS) + \" Fold \" + str(fold)) \n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \"\"\"name = model_name[:-4]\n",
    "        plot_name = name + \".png\"\n",
    "        plt.savefig(plot_name)\"\"\"\n",
    "        \n",
    "        tps = 0\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        F1s = []\n",
    "        IOUs = []\n",
    "\n",
    "        eval_model = UNET(3, 2)\n",
    "        eval_model = best_model\n",
    "\n",
    "        eval_model.eval()\n",
    "        for image, mask, name in testing_loader:\n",
    "            image_name = name[0]\n",
    "            prediction = predict_img(eval_model, transform(image), DEVICE, untransform, out_threshold=cst.THRESHOLD)\n",
    "            pred = torch.from_numpy(prediction)\n",
    "\n",
    "            precisions.append(metrics.precision(pred, mask))\n",
    "            recalls.append(metrics.recall(pred, mask))\n",
    "            F1s.append(metrics.F1Score(pred, mask))\n",
    "            IOUs.append(metrics.IOUScore(pred, mask))\n",
    "        \n",
    "        mean_precision = np.mean(precisions)\n",
    "        mean_recall = np.mean(recalls)\n",
    "        mean_f1 = np.mean(F1s)\n",
    "        mean_IOU = np.mean(IOUs)\n",
    "        \n",
    "        fold_precision.append(mean_precision)\n",
    "        fold_recall.append(mean_recall)\n",
    "        fold_f1.append(mean_f1)\n",
    "        fold_IOU.append(mean_IOU)\n",
    "        \n",
    "        confidence = 0.9\n",
    "        \n",
    "        print(\"--------------------\")\n",
    "        print(\"Term: \" + TERM)\n",
    "        print(\"Fold: {}\".format(fold))\n",
    "        print(\"Fold took: \" + str(minutes) + \" minutes \" + str(secondes) + \" seconds to train\")\n",
    "        print(\"Last val: {}\".format(loss))\n",
    "        print(\"Best val: {}\".format(best_val))\n",
    "        print()\n",
    "        print(\"Precision: {}\".format(mean_precision))\n",
    "        print(\"90% CI: {}\".format(np.percentile(precisions, [100*(1-confidence)/2,100*(1-(1-confidence)/2)])))\n",
    "        print(\"Min, max:\", np.min(precisions), np.max(precisions))\n",
    "        print()\n",
    "\n",
    "        print(\"Recall: {}\".format(mean_recall))\n",
    "        print(\"90% CI: {}\".format(np.percentile(recalls, [100*(1-confidence)/2,100*(1-(1-confidence)/2)])))\n",
    "        print(\"Min, max:\", np.min(recalls), np.max(recalls))\n",
    "        print()\n",
    "\n",
    "        print(\"F1/Dice score: {}\".format(mean_f1))\n",
    "        print(\"90% CI: {}\".format(np.percentile(F1s, [100*(1-confidence)/2,100*(1-(1-confidence)/2)])))\n",
    "        print(\"Min, max:\", np.min(F1s), np.max(F1s))\n",
    "        print()\n",
    "\n",
    "        print(\"IoU: {}\".format(mean_IOU))\n",
    "        print(\"90% CI: {}\".format(np.percentile(IOUs, [100*(1-confidence)/2,100*(1-(1-confidence)/2)])))\n",
    "        print(\"Min, max:\", np.min(IOUs), np.max(IOUs))\n",
    "        print()\n",
    "        print(\"--------------------\")\n",
    "    \"\"\"Fold loop end\"\"\"\n",
    "    print()\n",
    "    print(\"ALL FOLDS TRAINING ENDED\")\n",
    "    print(\"Mean best validation: {}\".format(np.mean(fold_validation)))\n",
    "    print(\"Mean precision: {}\".format(np.mean(fold_precision)))\n",
    "    print(\"Mean recall: {}\".format(np.mean(fold_recall)))\n",
    "    print(\"Mean F1: {}\".format(np.mean(fold_f1)))\n",
    "    print(\"Mean IOU: {}\".format(np.mean(fold_IOU)))\n",
    "\"\"\"term loop end\"\"\"\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
